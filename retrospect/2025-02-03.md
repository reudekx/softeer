# 2025-02-03 (5주차 월요일) 회고

## Review

> * Dano Lee 강사님께서 Spark 강의를 진행하셨다.
> * 또한 5주차 미션 1을 진행했다.

RDD, DAG, Stage, Job, Task, Transformation, Action.. 등 많은 것에 대한 설명을 들었다.
일단 암기를 한 뒤 과제를 수행하며 개념을 익혀나가도록 하자.

---

pyspark의 DataFrame 객체가 아닌 RDD 객체를 이용하여 Spark 과제를 수행하였다.

DataFrame에서는 사용자가 코드로써 작성한 여러 연산(Transformation 및 Action)에 대해 실행 계획을 세워 최적화를 수행하지만
그보다 저수준인 RDD에서는 이러한 최적화가 자동적으로 이뤄지지 않는 것 같았다. (이 서술에 대해서는 검증이 필요하다.)

RDD API를 통해 Spark를 다뤄보니 어떤 연산이 효율적이고/효율적이지 않은지 직접 확인을 할 수 있었다.
또한 Job들의 DAG를 확인해 본 결과, 가령 동일한 파일에 대한 read가 캐싱이 되지 않은 채 여러 번 이뤄지기도 했는데,
아마 RDD API를 이용하다 보니 실행 계획이 제대로 잡히지 않은 것이 아닐까? (마찬가지로 표현을 제대로 한 건지 모르겠다 찾아봐야 한다.)

---

`job history server`를 실행시키고 `spark-submit` 시 `--conf spark.eventLog.enabled=true` 옵션을 주어
Job이 종료된 이후에도 Web UI를 통해 과정과 결과를 확인할 수 있었다.

## Keep

* AI에 대한 의존을 조금씩 줄여나가고 있어 학습 효율이 더 좋아지고 있다.
  * 애초에 생각 없이 남용했던 것이 문제였고, 필요할 때 적절히 잘 사용할 수 있는 능력을 길러야 한다.

## Problem

* 기초에 대한 학습이 부족한 것 같다.
  * 알파벳을 외우지도 않고 영어 작문을 시도하고 있는 것 같다.

## Try

* Spark의 API에 대한 사용을 어느정도 자유롭게 할 수 있도록 하자.
  * 인터넷에서 tutorial 사이트를 찾아서 빠르게 A-Z까지 따라해 보자. 매일 아침에 30분씩 하면 좋을 것 같다.
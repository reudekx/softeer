# 2025-01-30 (4주차 목요일) 회고

## Review

> * 4주차 미션 2를 진행해 보자. (진행하며 작성 중)

**1. 먼저 요구사항을 분석해야 한다.**

- The application should be able to ingest multiple months or years of data efficiently.
- Handle different file formats such as CSV, Parquet, etc.

링크로 첨부된 사이트에 접속해서 어떤 종류의 데이터가 제공되고 있는지 확인해 보았다.

- Yellow Taxi Trip Records (PARQUET)
- Green Taxi Trip Records (PARQUET)
- For-Hire Vehicle Trip Records (PARQUET)
- High Volume For-Hire Vehicle Trip Records (PARQUET)

**언제부터의 데이터를 수집할 것인지?**

 2019년 2월부터 현재까지는 위 4가지 항목에 대한 데이터를 월 단위로 제공하고 있다.

그 전의 기간에 대해서는 특정 항목이 없는 경우가 있고, 가장 오래된 2009년의 경우 Yellow Taxi Trip Records 데이터만이 존재했다.

`피크 시간 분석`, `기후 조건 분석`을 하기 위해서는 최대한 많은 데이터를 수집하는 것이 유리해 보이면서도 현재의 경향을 잘 분석하기 위해서는 오히려 예전의 데이터가 방해가 될 수도 있다.

동시에 코로나19로 인한 여행 패턴의 변화가 분석 결과에 어떤 영향을 끼칠지도 고민해 봐야겠다는 생각이 들었다.

일단 **2019년 2월부터**의 데이터를 수집하는 것으로 하고, 최대한 확장을 고려해 코드를 작성하여 나중에 필요하다면 그 이전의 데이터도 활용할 수 있도록 하자.

**어떤 데이터를 수집할 것인지?**

- Peak Hours Identification:
  - Identify the peak hours for taxi usage.
- Weather Condition Analysis:
  - Analyze the effect of weather conditions on taxi demand. Use additional datasets if necessary to obtain weather information for the corresponding time periods.

택시 수요에 대한 분석을 진행해야 하므로, `Yellow Taxi`와 `Green Taxi`데이터를 수집하도록 하자. 물론 나머지 항목도 분석에 이용할 수 있겠지만, 데이터 처리에 드는 비용과 시간은 공짜가 아니므로.. 효율적인 방법이 무엇인지 잘 생각해야 한다. 따라서 일단 **택시 여행 기록**만 수집하도록 하자.

이후 Extract 코드를 작성한 결과 잘 작동하였다.

---

**2. 데이터 처리를 어떻게 진행할 것인가?**

과제에는 데이터 정리와 변환에 대한 요구사항도 명시되어 있었다.

- Handle missing values by either removing or imputing them.
- Convert all relevant time fields to a standard timestamp format.
- Filter out records with non-sensical values (e.g., negative trip duration or distance).

결측치와 이상치를 구태여 제거 혹은 대체할 것을 명시한 이유는 무엇일까?

아마도 추출한 데이터를 전처리하여 저장해 놓고, 나중에 분석을 위한 변환을 할 때 항상 정상적인 값들만 다룰 수 있도록 하기 위해서인 것 같다.

---

**코드 작성에 앞서 떠오른 의문을 먼저 정리해 보자.**

* MapReduce를 이용할 때는, HDFS에 데이터를 집어넣어, 입력 데이터가 이미 물리적으로 분산이 되어 있었다.

  * 현재 미션에서는 어떠한가?

  * 물론 Spark와 HDFS를 같이 사용할 수도 있지만, 지금은 단순히 볼륨 마운트를 통해 입력 데이터를 Spark 컨테이너들과 공유한다.
  * pyspark로 코드를 작성할 때 데이터에 대한 분산 처리가 명시적으로 잘 드러나는 것 같지는 않다.
  * 그렇다면 pyspark 라이브러리 혹은 Spark가 알아서 분산 처리를 하는 것일까? 사실 `pandas`를 사용할 때도 `DataFrame` 객체에 대한 연산을 수행하니 멀티 코어를 사용하는 것을 확인했던 적이 있다.
    * 정확히 누가 언제 데이터를 나누고 종합하는 것인지 찾아볼 필요가 있다.

**AI에게서 얻은 답변은 다음과 같다. 물론 학습을 하며 교차 검증을 해야 한다.**

* Spark:

	* 입력 데이터를 자동으로 분할
	* 각 Executor에 작업 할당
	* 필요한 경우 데이터 재분배(shuffle)
	* 결과 취합 및 저장

* pyspark가 따로 Spark의 분산 처리를 도와주는 것은 아닌 듯. 이미 Spark의 기능으로서 잘 구현이 되어 있다.

또한 인터넷 검색을 하다가 RDD(Resillient Distributed Data)라는 개념도 알게 되었다. 다음 문서를 통해 학습하자.

* https://spark.apache.org/docs/latest/rdd-programming-guide.html

---

일단 오늘은 여기까지 진행하고 남은 부분은 내일 해야겠다.

## Keep

* 기록과 함께 학습을 하니 체계적으로 과제를 수행할 수 있었다.

## Problem

* 계획을 짠 뒤 하루를 보내기로 했는데.. 까먹었다.
* Problem & Try를 열심히 적고는 있는데, 적은 사항을 까먹거나 실천하지 않는 것은 어떻게 해결해야 할지?

## Try

* 혼자 하는 것이 어렵다면 주위 사람들과 같이 해보자.
  * 각자 적은 Keep & Problem & Try에 대해 이야기를 해보도록 하자.


# 2025-01-29 (4주차 수요일) 회고

## Review

> * Spark를 설치 및 실행하여 M1을 진행해보자. (진행하며 작성 중)

**0. 시작에 앞서 미션을 파악해보자**

```
1. 1개의 마스터 노드와 2개의 워커 노드로 구성된 standalone 클러스터를 실행해야 한다.
2. Spark Job은 input 경로에서 데이터를 읽고 output 경로에 결과를 써야 한다.
   * 이 과정에서 CSV, parquet 등으로 결과를 저장해야 한다.
```

**미션을 읽고 난 뒤 다음의 막연한 의문들이 떠올랐다.**

* Spark는 YARN과 같은 자원 관리 계층의 요소와의 통합을, 내가 작성한 Spark 코드의 수정 없이 추후 필요할 시 쉽게 해낼 수 있는가? 물론 그래야 할 것 같긴 하지만, 정말 그럴까..
* 지난 주 미션에서 작성했던 Hadoop Streaming 방식을 이용한 MapReduce 사용 코드의 출력 결과는 어떤 형태로 HDFS에 저장되었던 것일까? 또한 CSV나 parquet을 HDFS에 저장할 수 있는 건가? 가능 여부와 별개로 HDFS에 데이터를 작성할 때 따로 형식이 있었던가.. 물론 input 데이터를 HDFS에 put할 때는 파일 형식을 그대로 넣긴 했었다. (물론 현재 미션에서는 HDFS를 사용하는 것이 아니라 그냥 linux 파일시스템(호스트 OS든 컨테이너 안이든..)에 저장하라는 것 같긴 하다) 저장 형식에 대한 제한은 딱히 없는 것인가? (그렇다고 생각하는 게 타당한 것 같다.)
  * 내가 지난 주 미션을 Word Count까지만 진행했기에 학습 과정에서 뭔가 놓친 것이 있을 수도 있다. 빠르게 Spark 미션을 끝내고 마치지 못했던 지난 Hadoop 미션을 다시 해야겠다.

**빠른 파악을 위해 AI에게 질문을 하여 답변을 얻었다. 물론 미션을 수행하며 검증을 해야 한다.**

* 그렇다. spark-submit 시 YARN의 사용 여부를 지정할 수 있는 것 보면, spark를 사용하는, 가령 파이썬 파일의 내부 코드를 수정할 필요가 없다.
    ```bash
    # Standalone 모드
    spark-submit --master spark://master:7077 program.py
    
    # YARN 모드
    spark-submit --master yarn program.py
    ```
    * 다만 standalone, yarn, ... 과 같은 cluster 자체의 구조는 이미 구성되어 있어야 하는 것이 아닌가? 즉 YARN을 이용하는 cluster를 구성했다면 제출 시 yarn만을 선택해야 한다.
    * 월요일 강의에서 `deploy mode`가 있다는 것을 배웠는데, 사실상 이미 구성된 cluster 위에서 자유롭게 선택할 수 있는 것은 `deploy mode` 뿐인 것 같다. 
* MapReduce의 결과를 단순히 stdout에 출력했다면 텍스트 형식으로 저장된다. 다만 HDFS는 말그대로 '파일 시스템'이니까, 거기에 어떤 형식의 파일을 저장할지는 별개의 문제다. 이번 미션에서는 parquet 파일을 HDFS에 저장하면 되는 것.

첫 의문에 대해서 좀 더 생각해보면, 결국 pyspark와 같이 사용자에게 특정 언어의 라이브러리 형태로 제공되는 인터페이스를 사용하게 되면 내부적으로 알아서 적절한 분산 처리를 한다는 것인가?

일단 미션을 수행하며 남은 의문들에 대한 답을 찾아보자.

---

**1. 도커 파일을 interactive하게 작성해 보자.**

지난 주 도커 파일을 작성하면서 많은 시간을 낭비했으므로.. 이번에는 같은 실수를 반복하지 말아야 한다.

저번에 도커 파일을 작성할 때는 막연히 확장성을 고려하고자 base 이미지로 ubuntu를 선택했었는데, 시간 단축을 위해 Java 정도는 기본적으로 깔려 있는 이미지를 선택해 보자. 애초에 한 컨테이너가 여러 기능을 담당하는 것이 별로 좋은 방향은 아닌 것 같고, 특정 버전의 Java가 설치된 이미지를 선택한다고 해서 확장성이 떨어질 이유도 딱히 없다. 

`eclipse-temurin:11-jre` 이미지를 이용하기로 했다. 어차피 컨테이너에서는 이미 빌드 혹은 컴파일된 자바 코드 및 파이썬 스크립트만을 실행하면 되므로, jdk는필요 없고 jre만 있으면 된다.

또한 Hadoop을 컴파일하기 위해서는 Java 8을 이용하는 것을 권장한다는데.. 마찬가지로 Hadoop 자체를 컴파일할 일이 (현재는) 없으므로 위 이미지로 결정했다.

---

**Spark 설치 방법을 찾아보다가, pyspark 같은 것들을 따로 설치해야 하는지 궁금해졌다. 검색을 해보니 Spark 다운 시 내부에 python 패키지들이 포함되어 있다고 하는데..**

* 지난번처럼 무작정 도커 파일을 작성 및 수정하는 게 아니라 직접 컨테이너 내부에서 다운로드 후 확인을 해보면 된다.
* 확인 결과 pyspark 패키지가 존재했다. 다만 python 인터프리터는 포함되어 있지 않아 따로 설치해줘야 한다는 것을 알게 되었다.

```
root@f501aa96ef6d:/usr/local/spark/python# ls
dist  MANIFEST.in  pyspark.egg-info  run-tests.py             setup.py
docs  mypy.ini     README.md         run-tests-with-coverage  test_coverage
lib   pyspark      run-tests         setup.cfg                test_support
root@f501aa96ef6d:/usr/local/spark/python# ls lib/
py4j-0.10.9.7-src.zip  PY4J_LICENSE.txt  pyspark.zip
```

`pyspark`가 두 군데에 존재했는데.. 검색을 해보니 python은 zip파일도 import를 할 수 있다고 한다. 어차피 `py4j`도 import를 해야하니, zip 파일이 존재하는 경로를 `PYTHONPATH` 환경변수에 추가하기로 하였다.

---

이제 Spark를 실행해 보자.

```bash
spark-submit $SPARK_HOME/examples/src/main/python/pi.py
```

위 명령어를 실행하니 아래와 같은 결과가 출력됐다. 옵션을 주지 않았더니 `local`모드로 실행되었다.

```bash
Pi is roughly 3.131000
```

조금 더 명시적인 명령어는 다음과 같다.

```bash
# 로컬 머신의 모든 코어를 사용하여 Spark를 local mode로 실행
spark-submit --master local[*] $SPARK_HOME/examples/src/main/python/pi.py
```

이로써 Spark가 정상적으로 설치되었음을 확인하였고, 실행했던 명령어들을 도커 파일에 옮겨 작성할 수 있게 되었다.

---

**2. docker-compose.yml을 작성하여 standalone cluster를 실행해 보자.**

시작에 앞서 궁금한 점이 생겼다.

* 각 노드들은 어떻게 통신하는지? 보안은 어떻게 달성되는가?
* Hadoop의 xml 형식의 설정 파일 같은 것이 존재하는지?

AI에게 질문을 하여 정보를 얻을 수 있었다.

* Master와 Worker는 Akka 프레임워크 기반의 RPC를 사용한다.
* Driver와 Executor는 Netty 기반의 RPC를 사용한다.
* 기본적으로는 암호화되지 않은 통신을 사용한다. 필요시 옵션을 줄 수 있다.

* `SPARK_HOME/conf` 디렉터리에 설정 파일이 존재한다.
  * 역시나 수많은 옵션들이 있다..

마찬가지로 풀리지 않은 의문은 미션을 수행하며 알아 보기로 하자.

---

**먼저 최소한의 설정을 구성한 뒤 실행을 해 보자.**

`spark-env.sh`에 어떤 내용을 적어야 할지는 다음 문서에 나타나있다. 필요한 환경 변수를 작성하면 된다.

* https://spark.apache.org/docs/latest/configuration.html#environment-variables
* https://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts

`spark-defaults.conf`에 적을 내용은 다음 문서를 확인하도록 하자.

* https://spark.apache.org/docs/latest/configuration.html#available-properties
* https://wikidocs.net/107317

위 문서들을 참고하여 적절히 설정 파일들을 작성했고, 컨테이너들을 실행할 수 있었다.

일단 볼륨 마운트는 하지 않았고, 직접 master container에 접속한 뒤 아래 명령어를 실행해 보았다.

```bash
spark-submit \
  --master spark://spark-master:7077 \
  --deploy-mode client \
  $SPARK_HOME/examples/src/main/python/pi.py 10
```

다행히 오류는 발생하지 않았고 다음 결과를 얻을 수 있었다.

```
Pi is roughly 3.145960
```

---

**3. 이제 남은 세부 요구사항을 구현해야 한다.**

* The Spark job should be able to read a dataset from a mounted volume or a specified input path.
* The job should perform a data transformation (e.g., estimating π) and write the results to a specified output path.
* The output should be correctly partitioned and saved in a format such as CSV or Parquet.

---

먼저 docker-compose.yml에 볼륨을 마운트하는 부분을 추가하자. (모든 컨테이너에 각각 추가해주었다.)

```yaml
volumes:
  - ${HOME}/docker/volumes/spark/data:${SPARK_HOME}/data
```

다음으로 제출할 Job에 해당하는 python 파일을 작성해야 한다. 과제에서도 언급되어 있는 예제 파일을 이용하기로 하였다. 

시험 삼아 노트북의 로컬 환경에 Java를 설치한 뒤 파이썬 파일을 실행봤는데.. 잘 작동하였다.

```
Pi is roughly 3.148200
```

뒤이어 Parquet 파일로 결과를 저장해 보기도 하였고, 도커에서 실행 중인 Spark cluster에서도 잘 실행되는 것을 확인할 수 있었다.

---

**4주차 첫번째 미션 완료 후기**

지난 주에 수많은 삽질과 시행착오를 겪고 많이 지쳤었는데, 그때의 경험을 바탕으로 개선해야 할 점을 잘 인지하며 미션을 수행했더니 지난 번보다 훨씬 수월하게 마칠 수 있었다.

남은 과제들도 오늘처럼 잘 해낼 수 있도록 더욱 노력해야겠다.

## Keep

* 과제를 수행하고 문제를 해결하는 요령을 점점 터득하고 있다.

## Problem

* 시간 관리를 더 철저히 해야할 필요가 있다.

## Try

* 내일의 내가 실천을 할지는 모르겠으나.. 하루를 시작하기 전 계획을 세운 뒤 그에 맞춰 공부를 해보도록 하자.
